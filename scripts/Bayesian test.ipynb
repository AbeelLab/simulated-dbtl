{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0be5913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_designs:  20\n",
      "scenario: radical\n",
      "N_runs: 1\n",
      "0\n",
      "RandomForestRegressor(max_depth=4, min_samples_leaf=5, min_samples_split=3)\n",
      "GradientBoostingRegressor(learning_rate=0.2, max_depth=1, min_samples_leaf=6,\n",
      "                          min_samples_split=3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "the lower bound 20 has to be less than the upper bound 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35851/1131930408.py\u001b[0m in \u001b[0;36m<cell line: 350>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_intersection_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_runs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_runs_averaging\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_designs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35851/1131930408.py\u001b[0m in \u001b[0;36mrun_intersection_benchmark\u001b[0;34m(N_runs, N_runs_averaging, topX, number_of_designs)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;31m#Get the top 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mtop100\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Enzyme_G'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtopX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mintersection_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_intersection_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mint_top100_sgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mint_top100_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35851/1131930408.py\u001b[0m in \u001b[0;36mget_intersection_scores\u001b[0;34m(train_X, train_Y, test_X, test_Y, topX)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Neural Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     regr_NN = BayesSearchCV(\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mMLPRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     {\n",
      "\u001b[0;32m~/anaconda3/envs/skimpy-env/lib/python3.8/site-packages/skopt/searchcv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, estimator, search_spaces, optimizer_kwargs, n_iter, scoring, fit_params, n_jobs, n_points, iid, refit, cv, verbose, pre_dispatch, random_state, error_score, return_train_score)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_search_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_spaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Temporary fix for compatibility with sklearn 0.20 and 0.21\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# See scikit-optimize#762\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/skimpy-env/lib/python3.8/site-packages/skopt/searchcv.py\u001b[0m in \u001b[0;36m_check_search_space\u001b[0;34m(self, search_space)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msubspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m                     \u001b[0mcheck_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m~/anaconda3/envs/skimpy-env/lib/python3.8/site-packages/skopt/space/space.py\u001b[0m in \u001b[0;36mcheck_dimension\u001b[0;34m(dimension, transform)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mReal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/skimpy-env/lib/python3.8/site-packages/skopt/space/space.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, low, high, prior, base, transform, name, dtype)\u001b[0m\n\u001b[1;32m    438\u001b[0m                  name=None, dtype=np.int64):\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhigh\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             raise ValueError(\"the lower bound {} has to be less than the\"\n\u001b[0m\u001b[1;32m    441\u001b[0m                              \" upper bound {}\".format(low, high))\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprior\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log-uniform\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: the lower bound 20 has to be less than the upper bound 20"
     ]
    }
   ],
   "source": [
    "from pytfa.io.json import load_json_model\n",
    "from skimpy.io.yaml import  load_yaml_model\n",
    "from skimpy.analysis.oracle.load_pytfa_solution import load_concentrations, load_fluxes\n",
    "from skimpy.core.parameters import ParameterValues\n",
    "from skimpy.utils.namespace import *\n",
    "from skimpy.core.modifiers import *\n",
    "from skimpy.io.yaml import load_yaml_model\n",
    "from skimpy.core.reactor import Reactor\n",
    "from skimpy.analysis.oracle.load_pytfa_solution import load_concentrations, load_fluxes\n",
    "from skimpy.viz.plotting import timetrace_plot\n",
    "from pytfa.io.json import load_json_model\n",
    "from skimpy.io.yaml import load_yaml_model\n",
    "from skimpy.analysis.oracle.load_pytfa_solution import load_concentrations\n",
    "from skimpy.core.parameters import load_parameter_population\n",
    "from skimpy.simulations.reactor import make_batch_reactor\n",
    "from skimpy.core.solution import ODESolutionPopulation\n",
    "from skimpy.utils.namespace import *\n",
    "from skimpy.viz.escher import animate_fluxes, plot_fluxes\n",
    "import copy\n",
    "from skimpy.io.yaml import export_to_yaml\n",
    "from skimpy.analysis.ode.utils import make_flux_fun\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import seaborn as sns\n",
    "import skimpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import matplotlib\n",
    "import sys\n",
    "sys.path.insert(1, '../functions/')\n",
    "\n",
    "# benchmark functions\n",
    "import simulation_functions as sf\n",
    "import scenarios as sc\n",
    "import visualizations as vis\n",
    "\n",
    "\n",
    "#ML methods\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import  AdaBoostRegressor\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "\n",
    "N_designs=20\n",
    "scenario=\"radical\"\n",
    "N_runs=1\n",
    "\n",
    "a=time.time()\n",
    "\n",
    "print(\"N_designs: \", N_designs)\n",
    "print(\"scenario:\", scenario)\n",
    "print(\"N_runs:\", N_runs)\n",
    "\n",
    "\n",
    "\n",
    "def get_intersection_scores(train_X,train_Y,test_X,test_Y,topX):\n",
    "    \"\"\"Calculates all the intersections for the number of runs, with bayesian optimization\"\"\"\n",
    "\n",
    "    # Get all the ML methods\n",
    "    #svr\n",
    "\n",
    "    #sgd\n",
    "    regr_sgd = make_pipeline(StandardScaler(),SGDRegressor(loss=\"squared_error\",max_iter=1000, tol=1e-3))\n",
    "    regr_sgd.fit(train_X, train_Y)\n",
    "    predict_sgd=regr_sgd.predict(test_X)\n",
    "\n",
    "    #rf\n",
    "    regr_rf = BayesSearchCV(\n",
    "    RandomForestRegressor(),\n",
    "    {\n",
    "        \"min_samples_split\":(2,3,4,5,6),\n",
    "        \"min_samples_leaf\":(2,3,4,5,6),\n",
    "        \"max_depth\": (1,2,3,4,5),\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=5)\n",
    "    regr_rf.fit(train_X,train_Y)\n",
    "    print(regr_rf.best_estimator_)\n",
    "    predict_rf=regr_rf.predict(test_X)\n",
    "\n",
    "\n",
    "    #Gradient boosting Regressor\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    regr_GradBoostReg = BayesSearchCV(\n",
    "    GradientBoostingRegressor(),\n",
    "    {\n",
    "        \"min_samples_split\":(2,3,4,5,6),\n",
    "        \"min_samples_leaf\":(2,3,4,5,6),\n",
    "        \"max_depth\": (1,2,3,4,5),\n",
    "        \"learning_rate\":(0.0001,0.001,0.01,0.1,0.2,0.3),\n",
    "        #'learning_rate': (0.01,0.2,0.4,0.6),\n",
    "        #'min_samples_split': (2,3,4)\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=5)\n",
    "    regr_GradBoostReg.fit(train_X, train_Y)\n",
    "    print(regr_GradBoostReg.best_estimator_)\n",
    "    predict_GradBoostReg=regr_GradBoostReg.predict(test_X)\n",
    "\n",
    "    # Neural Network\n",
    "    regr_NN = BayesSearchCV(\n",
    "    MLPRegressor(),\n",
    "    {\n",
    "        \"activation\":(\"logistic\",\"tanh\",\"relu\"),\n",
    "        \"alpha\":(0.0001,0.01),\n",
    "        \"max_iter\":(5000,8000),\n",
    "        \"hidden_layer_sizes\": ((20,20)),\n",
    "        \n",
    "        \"learning_rate\":(\"invscaling\",\"constant\",\"adaptive\"),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=5)\n",
    "    regr_NN.fit(train_X,train_Y)\n",
    "    print(regr_NN.best_estimator_)\n",
    "    predict_NN=regr_NN.predict(test_X)\n",
    "    \n",
    "    top100=np.argsort(test_Y)[::-1][0:topX]\n",
    "    \n",
    "    top100_sgd=np.argsort(predict_sgd)[::-1][0:topX]\n",
    "    top100_rf=np.argsort(predict_rf)[::-1][0:topX]\n",
    "    top100_gbr=np.argsort(predict_GradBoostReg)[::-1][0:topX]\n",
    "    top100_nn=np.argsort(predict_NN)[::-1][0:topX]\n",
    "    \n",
    "    \n",
    "    top100_sgd=len(np.intersect1d(top100,top100_sgd))\n",
    "    top100_rf=len(np.intersect1d(top100,top100_rf))\n",
    "    top100_gbr=len(np.intersect1d(top100,top100_gbr))\n",
    "    top100_nn=len(np.intersect1d(top100,top100_nn))\n",
    "    \n",
    "    int_scores=[top100_sgd,top100_rf,top100_gbr,top100_nn]\n",
    "    return int_scores\n",
    "\n",
    "\n",
    "def ML_methods(train_x,train_y,test_x,test_y):\n",
    "    \"wrapper s.t. we can call it iteratively for each scenario\"\n",
    "    #train test split\n",
    "    #X=['vmax_forward_Enzyme_A','vmax_forward_Enzyme_B',\"vmax_forward_Enzyme_C\",\n",
    "     #  \"vmax_forward_Enzyme_D\",\"vmax_forward_Enzyme_E\",\"vmax_forward_Enzyme_F\",\"vmax_forward_Enzyme_G\"]\n",
    "    #train_x=training_set_sc1[X]\n",
    "    #train_y=training_set_sc1['Enzyme_G']\n",
    "    #test_x=test_set_simulation[X]\n",
    "    #test_y=test_set_simulation['Enzyme_G']\n",
    "    \n",
    "    \n",
    "    #score_svr=regr_svr.score(test_x,test_y)\n",
    "    #print(\"Support Vector Regressor: \"+str(regr_svr.score(test_x,test_y)))\n",
    "    #sgd\n",
    "    regr_sgd = make_pipeline(StandardScaler(),SGDRegressor(loss=\"squared_error\",max_iter=1000, tol=1e-3))\n",
    "    regr_sgd.fit(train_x, train_y)\n",
    "    predict_sgd=regr_sgd.predict(test_x)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(test_y,predict_sgd)\n",
    "    score_sgd=r_value**2\n",
    "    \n",
    "\n",
    "    #print(\"Stochastic Gradient Descent Regressor: \"+str(regr_sgd.score(test_x,test_y)))\n",
    "    #rf\n",
    "    regr_rf = BayesSearchCV(\n",
    "    RandomForestRegressor(),\n",
    "    {\n",
    "        \"min_samples_split\":(2,3,4,5,6),\n",
    "        \"min_samples_leaf\":(2,3,4,5,6),\n",
    "        \"max_depth\": (1,2,3,4,5),\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=5)\n",
    "    regr_rf.fit(train_X,train_Y)\n",
    "    print(regr_rf.best_estimator_)\n",
    "    predict_rf=regr_rf.predict(test_X)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(test_y,predict_rf)\n",
    "    score_rf=r_value**2\n",
    "\n",
    "    \n",
    "    #Gradient boosting Regressor\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    regr_GradBoostReg = BayesSearchCV(\n",
    "    GradientBoostingRegressor(),\n",
    "    {\n",
    "        \"min_samples_split\":(2,3,4,5,6),\n",
    "        \"min_samples_leaf\":(2,3,4,5,6),\n",
    "        \"max_depth\": (1,2,3,4,5),\n",
    "        \"learning_rate\":(0.0001,0.001,0.01,0.1,0.2,0.3),\n",
    "        #'learning_rate': (0.01,0.2,0.4,0.6),\n",
    "        #'min_samples_split': (2,3,4)\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=5)\n",
    "    \n",
    "    regr_GradBoostReg.fit(train_X, train_Y)\n",
    "    print(regr_GradBoostReg.best_estimator_)\n",
    "    predict_GradBoostReg=regr_GradBoostReg.predict(test_x)\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = linregress(test_y,predict_GradBoostReg)\n",
    "    score_GradBoost=r_value**2\n",
    "    \n",
    "    #print(\"Gradient Boosting Regressor: \"+str(regr_GradBoostReg.score(test_x,test_y))) \n",
    "    # Neural Network\n",
    "    regr_NN = BayesSearchCV(\n",
    "    MLPRegressor(),\n",
    "    {\n",
    "        \"activation\":(\"logistic\",\"tanh\",\"relu\"),\n",
    "        \"alpha\":(0.0001,0.01),\n",
    "        \"max_iter\":(5000,8000),\n",
    "        \"hidden_layer_sizes\": (1,20),\n",
    "        \"learning_rate\":(\"invscaling\",\"constant\",\"adaptive\"),\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=5)\n",
    "    regr_NN.fit(train_X,train_Y)\n",
    "    print(regr_NN.best_estimator_)\n",
    "    predict_NN=regr_NN.predict(test_x)\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = linregress(test_y,predict_NN)\n",
    "    score_NN=r_value**2\n",
    "    \n",
    "    #print(\"Neural Network: \"+str(regr_NN.score(test_x,test_y))) \n",
    "    algorithms=[\"SGD\",\"RF\",\"GBR\",\"NN\"]\n",
    "    scores=[score_sgd,score_rf,score_GradBoost,score_NN]\n",
    "    scores=dict(zip(algorithms,scores))\n",
    "    return scores \n",
    "   \n",
    "def run_intersection_benchmark(N_runs,N_runs_averaging,topX, number_of_designs):\n",
    "    \"\"\"Wrapper for a specific number of designs\n",
    "    - INput:\n",
    "    1) N_runs to average: calculate the intersection x times and average\n",
    "    2) N_runs: number of times the average is calculated\n",
    "    3) Number of designs: number of designs in the training set\n",
    "    4) output: dataframe with all the intersections\"\"\"\n",
    "\n",
    "    top100_sgd=[]\n",
    "    top100_rf=[]\n",
    "    top100_gbr=[]\n",
    "    top100_nn=[]\n",
    "\n",
    "    for i in range(N_runs):   \n",
    "        print(i)\n",
    "        int_top100_sgd=[]\n",
    "        int_top100_rf=[]\n",
    "        int_top100_gbr=[]\n",
    "        int_top100_nn=[]\n",
    "        for i in range(N_runs_averaging):\n",
    "            N_designs=number_of_designs\n",
    "            #Define the scenario \n",
    "            if scenario==\"equal\":\n",
    "                sc1_designs,sc1_cart=sc.scenario1(perturb_range,N_designs,enz_names)\n",
    "            elif scenario==\"radical\":\n",
    "                sc1_designs,sc1_cart=sc.scenario2(perturb_range,N_designs,enz_names)\n",
    "            elif scenario==\"non-radical\":\n",
    "                sc1_designs,sc1_cart=sc.scenario3(perturb_range,N_designs,enz_names)\n",
    "            #Retrieve the training set instances\n",
    "            training_scenario1,training_cart=find_set_designs(comb_space,sc1_cart,enz_names) \n",
    "\n",
    "            #Get the test set instances (this function is re-used from the script \n",
    "            #where the combinatorial space is not available and therefoer requires finding the instances in the comb\n",
    "            #space again REWRITE)\n",
    "            #test_cart=test_unseen_designs(cart,enz_names,4000)\n",
    "            #test_scenario1,test_cart=find_set_designs(comb_space,test_cart,enz_names) \n",
    "\n",
    "            train_X=training_scenario1[enz_names]\n",
    "            train_Y=training_scenario1['Enzyme_G']\n",
    "            test_X=comb_space[enz_names]\n",
    "            test_Y=comb_space['Enzyme_G']\n",
    "\n",
    "            #Get the top 100\n",
    "            top100=np.argsort(comb_space['Enzyme_G'])[::-1][0:topX]\n",
    "            intersection_scores=get_intersection_scores(train_X,train_Y,test_X,test_Y,topX)\n",
    "            int_top100_sgd.append(intersection_scores[0])\n",
    "            int_top100_rf.append(intersection_scores[1])\n",
    "            int_top100_gbr.append(intersection_scores[2])\n",
    "            int_top100_nn.append(intersection_scores[3])\n",
    "\n",
    "        int_top100_sgd=np.mean(int_top100_sgd)\n",
    "        int_top100_rf=np.mean(int_top100_rf)\n",
    "        int_top100_gbr=np.mean(int_top100_gbr)\n",
    "        int_top100_nn=np.mean(int_top100_nn)\n",
    "\n",
    "\n",
    "        top100_sgd.append(int_top100_sgd)\n",
    "        top100_rf.append(int_top100_rf)\n",
    "        top100_gbr.append(int_top100_gbr)\n",
    "        top100_nn.append(int_top100_nn)\n",
    "    results_intersection1000={\"SGD\":top100_sgd,\"RF\":top100_rf,\n",
    "                            \"GBR\":top100_gbr,\"NN\":top100_nn}\n",
    "    results_intersection1000=pd.DataFrame(results_intersection1000)\n",
    "    return results_intersection1000\n",
    "\n",
    "def test_unseen_designs(cart,enz_names,test_set_size):\n",
    "    \"\"\"testing predictions from \"\"\"\n",
    "    index_set=np.arange(0,len(cart),1)\n",
    "    random_choice=np.random.choice(index_set,test_set_size,replace=False)\n",
    "    test_cart=[cart[i] for i in random_choice]\n",
    "    return test_cart\n",
    "\n",
    "\n",
    "def find_set_designs(comb_space,tcart,enz_names):\n",
    "    \"\"\"finds the training or test set designs in the combinatorial space\n",
    "    Number of features has to be given\n",
    "    - combinatorial space\n",
    "    - cart of either the training scenario or the test set\n",
    "    - enzyme names\"\"\"\n",
    "    temp=0\n",
    "    tset = pd.DataFrame()\n",
    "    for design in tcart:\n",
    "        sub=comb_space\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_A']==design[0]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_B']==design[1]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_C']==design[2]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_D']==design[3]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_E']==design[4]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_F']==design[5]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_G']==design[6]]\n",
    "        tset=pd.concat([tset,sub])\n",
    "    return tset,tcart\n",
    "\n",
    "\n",
    "\n",
    "#Load simulations\n",
    "comb_space=pd.read_csv(\"../data/combinatorial_space/combinatorial_space_pathway_A.csv\")\n",
    "\n",
    "#enzyme names and perturbation range\n",
    "enz_names=[\"vmax_forward_Enzyme_A\",\"vmax_forward_Enzyme_B\",\"vmax_forward_Enzyme_C\",\"vmax_forward_Enzyme_D\",\n",
    "           \"vmax_forward_Enzyme_E\",\"vmax_forward_Enzyme_F\",\"vmax_forward_Enzyme_G\"] #'vmax_forward_LDH_D',\n",
    "perturb_range=[0.25,0.5,1,1.5,2,4]\n",
    "designs,cart=sf.generate_perturbation_scheme(enz_names,perturb_range)\n",
    "\n",
    "\n",
    "\n",
    "N_runs_averaging=3\n",
    "topX=100\n",
    "a=time.time()\n",
    "\n",
    "\n",
    "results=run_intersection_benchmark(N_runs,N_runs_averaging,topX, N_designs)\n",
    "b=time.time()\n",
    "print(b-a)\n",
    "results=pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d03b6018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SGD</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBR</th>\n",
       "      <th>NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SGD        RF       GBR        NN\n",
       "0  13.333333  0.666667  0.666667  1.666667"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db900d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SGD</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBR</th>\n",
       "      <th>NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SGD   RF  GBR         NN\n",
       "0  18.0  4.0  0.0  17.666667"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a172dc",
   "metadata": {},
   "source": [
    "# Identify parameters for bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e9ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd\n",
    "regr_sgd = make_pipeline(StandardScaler(),SGDRegressor(loss=\"squared_error\",max_iter=1000, tol=1e-3))\n",
    "#regr_sgd.fit(train_x, train_y)\n",
    "#predict_sgd=regr_sgd.predict(test_X)\n",
    "\n",
    "#rf\n",
    "regr_rf = RandomForestRegressor()\n",
    "#regr_rf.fit(train_X, train_Y)\n",
    "#predict_rf=regr_rf.predict(test_X)\n",
    "\n",
    "#Elastic Net\n",
    "regr_elasticnet=ElasticNet()\n",
    "#regr_elasticnet.fit(train_X,train_Y)\n",
    "#predict_elasticnet=regr_elasticnet.predict(test_X)\n",
    "\n",
    "\n",
    "#Gradient boosting Regressor\n",
    "regr_GradBoostReg=GradientBoostingRegressor()\n",
    "#regr_GradBoostReg.fit(train_X,train_Y)\n",
    "#predict_GradBoostReg=regr_GradBoostReg.predict(test_X)\n",
    "\n",
    "# Neural Network\n",
    "regr_NN=MLPRegressor(max_iter=8000,activation=\"relu\", learning_rate=\"adaptive\")\n",
    "#regr_NN.fit(train_X,train_Y)\n",
    "#predict_NN=regr_NN.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "396d8f47",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1869934973.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_35851/1869934973.py\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    \"activation\":(\"logistic\",\"tanh\",\"reluâ€™),\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "regr_sgd.get_params()\n",
    "#no real parameters to set for this one, is linear\n",
    "\n",
    "regr_GradBoostReg.get_params()\n",
    "# min_samples_split (2), min_samples_leaf (1), max_depth (3),learning rate (0.001,0.01,0.1,0.2),n_estmators (100),\n",
    "\n",
    "regr_rf.get_params()\n",
    "# min_samples_split (2), min_samples_leaf (1), max_depth (3),n_estimators (100),\n",
    "\n",
    "regr_NN.get_params()\n",
    "#activation: 'relu',alpha ()\n",
    "#parameter_space = {\n",
    "#    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "#    'activation': ['tanh', 'relu'],\n",
    "#    'solver': ['sgd', 'adam'],\n",
    "#    'alpha': [0.0001, 0.05],\n",
    "#    'learning_rate': ['constant','adaptive'],\n",
    "#}\n",
    "\n",
    "\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "regr_NN = BayesSearchCV(\n",
    "    MLPRegressor(),\n",
    "    {\n",
    "        \"activation\":(\"logistic\",\"tanh\",\"relu\"),\n",
    "        \"alpha\":(0.0001,0.001,0.01),\n",
    "        \"hidden_layer_sizes\": ((7,),(7,7),(7,7,7,),(7,3),(5,3,),(3,3,)),\n",
    "        \"learning_rate\":(0.001,0.01,0.1,0.2,0.3),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=5)\n",
    "\n",
    "regr_GradBoostReg.fit(train_x, train_y)\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e871fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
