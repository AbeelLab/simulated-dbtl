{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0be5913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling...\n",
      " A: min|aij| =  1.000e+00  max|aij| =  1.000e+00  ratio =  1.000e+00\n",
      "Problem data seem to be well scaled\n",
      "N_designs:  10\n",
      "scenario: radical\n",
      "N_runs: 2\n"
     ]
    }
   ],
   "source": [
    "from pytfa.io.json import load_json_model\n",
    "from skimpy.io.yaml import  load_yaml_model\n",
    "from skimpy.analysis.oracle.load_pytfa_solution import load_concentrations, load_fluxes\n",
    "from skimpy.core.parameters import ParameterValues\n",
    "from skimpy.utils.namespace import *\n",
    "from skimpy.core.modifiers import *\n",
    "from skimpy.io.yaml import load_yaml_model\n",
    "from skimpy.core.reactor import Reactor\n",
    "from skimpy.analysis.oracle.load_pytfa_solution import load_concentrations, load_fluxes\n",
    "from skimpy.viz.plotting import timetrace_plot\n",
    "from pytfa.io.json import load_json_model\n",
    "from skimpy.io.yaml import load_yaml_model\n",
    "from skimpy.analysis.oracle.load_pytfa_solution import load_concentrations\n",
    "from skimpy.core.parameters import load_parameter_population\n",
    "from skimpy.simulations.reactor import make_batch_reactor\n",
    "from skimpy.core.solution import ODESolutionPopulation\n",
    "from skimpy.utils.namespace import *\n",
    "from skimpy.viz.escher import animate_fluxes, plot_fluxes\n",
    "import copy\n",
    "from skimpy.io.yaml import export_to_yaml\n",
    "from skimpy.analysis.ode.utils import make_flux_fun\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import seaborn as sns\n",
    "import skimpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import matplotlib\n",
    "import sys\n",
    "sys.path.insert(1, '../functions/')\n",
    "\n",
    "# benchmark functions\n",
    "import simulation_functions as sf\n",
    "import scenarios as sc\n",
    "import visualizations as vis\n",
    "\n",
    "\n",
    "#ML methods\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import  AdaBoostRegressor\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "\n",
    "N_designs=10\n",
    "scenario=\"radical\"\n",
    "N_runs=2\n",
    "\n",
    "\n",
    "\n",
    "a=time.time()\n",
    "\n",
    "print(\"N_designs: \", N_designs)\n",
    "print(\"scenario:\", scenario)\n",
    "print(\"N_runs:\", N_runs)\n",
    "\n",
    "\n",
    "\n",
    "def get_intersection_scores(train_X,train_Y,test_X,test_Y,topX):\n",
    "    \"\"\"Calculates all the intersections for the number of runs, with bayesian optimization\"\"\"\n",
    "\n",
    "    # Get all the ML methods\n",
    "    #svr\n",
    "\n",
    "    #sgd\n",
    "    regr_sgd = make_pipeline(StandardScaler(),SGDRegressor(loss=\"squared_error\",max_iter=1000, tol=1e-3))\n",
    "    regr_sgd.fit(train_X, train_Y)\n",
    "    predict_sgd=regr_sgd.predict(test_X)\n",
    "\n",
    "    #rf\n",
    "    regr_rf = RandomForestRegressor()\n",
    "    regr_rf.fit(train_X, train_Y)\n",
    "    predict_rf=regr_rf.predict(test_X)\n",
    "\n",
    "    #Elastic Net\n",
    "    regr_elasticnet=ElasticNet()\n",
    "    regr_elasticnet.fit(train_X,train_Y)\n",
    "    predict_elasticnet=regr_elasticnet.predict(test_X)\n",
    "\n",
    "\n",
    "    #Gradient boosting Regressor\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    regr_GradBoostReg = BayesSearchCV(\n",
    "    GradientBoostingRegressor(),\n",
    "    {\n",
    "        \"min_samples_split\":(1,2,3,4,5,6),\n",
    "        \"min_samples_leaf\":(1,2,3,4,5,6),\n",
    "        \"hidden_layer_sizes\": (1,2,3,4,5),\n",
    "        \"learning_rate\":(0.001,0.01,0.1,0.2,0.3),\n",
    "        #'learning_rate': (0.01,0.2,0.4,0.6),\n",
    "        #'min_samples_split': (2,3,4)\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=5)\n",
    "\n",
    "    regr_GradBoostReg.fit(train_x, train_y)\n",
    "    predict_GradBoostReg=regr_GradBoostReg.predict(test_X)\n",
    "\n",
    "    # Neural Network\n",
    "    regr_NN=MLPRegressor(max_iter=8000,activation=\"relu\", learning_rate=\"adaptive\")\n",
    "    regr_NN.fit(train_X,train_Y)\n",
    "    predict_NN=regr_NN.predict(test_X)\n",
    "    \n",
    "    top100=np.argsort(test_Y)[::-1][0:topX]\n",
    "    \n",
    "    top100_sgd=np.argsort(predict_sgd)[::-1][0:topX]\n",
    "    top100_rf=np.argsort(predict_rf)[::-1][0:topX]\n",
    "    top100_gbr=np.argsort(predict_GradBoostReg)[::-1][0:topX]\n",
    "    top100_nn=np.argsort(predict_NN)[::-1][0:topX]\n",
    "    \n",
    "    \n",
    "    top100_sgd=len(np.intersect1d(top100,top100_sgd))\n",
    "    top100_rf=len(np.intersect1d(top100,top100_rf))\n",
    "    top100_gbr=len(np.intersect1d(top100,top100_gbr))\n",
    "    top100_nn=len(np.intersect1d(top100,top100_nn))\n",
    "    \n",
    "\n",
    "    int_scores=[top100_sgd,top100_rf,top100_gbr,top100_nn]\n",
    "    return int_scores\n",
    "\n",
    "\n",
    "   \n",
    "def run_intersection_benchmark(N_runs,N_runs_averaging,topX, number_of_designs):\n",
    "    \"\"\"Wrapper for a specific number of designs\n",
    "    - INput:\n",
    "    1) N_runs to average: calculate the intersection x times and average\n",
    "    2) N_runs: number of times the average is calculated\n",
    "    3) Number of designs: number of designs in the training set\n",
    "    4) output: dataframe with all the intersections\"\"\"\n",
    "\n",
    "    top100_sgd=[]\n",
    "    top100_rf=[]\n",
    "    top100_gbr=[]\n",
    "    top100_nn=[]\n",
    "\n",
    "    for i in range(N_runs):   \n",
    "        print(i)\n",
    "        int_top100_sgd=[]\n",
    "        int_top100_rf=[]\n",
    "        int_top100_gbr=[]\n",
    "        int_top100_nn=[]\n",
    "        for i in range(N_runs_averaging):\n",
    "            N_designs=number_of_designs\n",
    "            #Define the scenario \n",
    "            if scenario==\"equal\":\n",
    "                sc1_designs,sc1_cart=sc.scenario1(perturb_range,N_designs,enz_names)\n",
    "            elif scenario==\"radical\":\n",
    "                sc1_designs,sc1_cart=sc.scenario2(perturb_range,N_designs,enz_names)\n",
    "            elif scenario==\"non-radical\":\n",
    "                sc1_designs,sc1_cart=sc.scenario3(perturb_range,N_designs,enz_names)\n",
    "            #Retrieve the training set instances\n",
    "            training_scenario1,training_cart=find_set_designs(comb_space,sc1_cart,enz_names) \n",
    "\n",
    "            #Get the test set instances (this function is re-used from the script \n",
    "            #where the combinatorial space is not available and therefoer requires finding the instances in the comb\n",
    "            #space again REWRITE)\n",
    "            #test_cart=test_unseen_designs(cart,enz_names,4000)\n",
    "            #test_scenario1,test_cart=find_set_designs(comb_space,test_cart,enz_names) \n",
    "\n",
    "            train_X=training_scenario1[enz_names]\n",
    "            train_Y=training_scenario1['Enzyme_G']\n",
    "            test_X=comb_space[enz_names]\n",
    "            test_Y=comb_space['Enzyme_G']\n",
    "\n",
    "            #Get the top 100\n",
    "            top100=np.argsort(comb_space['Enzyme_G'])[::-1][0:topX]\n",
    "            intersection_scores=get_intersection_scores(train_X,train_Y,test_X,test_Y,topX)\n",
    "            int_top100_sgd.append(intersection_scores[0])\n",
    "            int_top100_rf.append(intersection_scores[1])\n",
    "            int_top100_gbr.append(intersection_scores[2])\n",
    "            int_top100_nn.append(intersection_scores[3])\n",
    "\n",
    "        int_top100_sgd=np.mean(int_top100_sgd)\n",
    "        int_top100_rf=np.mean(int_top100_rf)\n",
    "        int_top100_gbr=np.mean(int_top100_gbr)\n",
    "        int_top100_nn=np.mean(int_top100_nn)\n",
    "\n",
    "\n",
    "        top100_sgd.append(int_top100_sgd)\n",
    "        top100_rf.append(int_top100_rf)\n",
    "        top100_gbr.append(int_top100_gbr)\n",
    "        top100_nn.append(int_top100_nn)\n",
    "    results_intersection1000={\"SGD\":top100_sgd,\"RF\":top100_rf,\n",
    "                            \"GBR\":top100_gbr,\"NN\":top100_nn}\n",
    "    results_intersection1000=pd.DataFrame(results_intersection1000)\n",
    "    return results_intersection1000\n",
    "\n",
    "def test_unseen_designs(cart,enz_names,test_set_size):\n",
    "    \"\"\"testing predictions from \"\"\"\n",
    "    index_set=np.arange(0,len(cart),1)\n",
    "    random_choice=np.random.choice(index_set,test_set_size,replace=False)\n",
    "    test_cart=[cart[i] for i in random_choice]\n",
    "    return test_cart\n",
    "\n",
    "\n",
    "def find_set_designs(comb_space,tcart,enz_names):\n",
    "    \"\"\"finds the training or test set designs in the combinatorial space\n",
    "    Number of features has to be given\n",
    "    - combinatorial space\n",
    "    - cart of either the training scenario or the test set\n",
    "    - enzyme names\"\"\"\n",
    "    temp=0\n",
    "    tset = pd.DataFrame()\n",
    "    for design in tcart:\n",
    "        sub=comb_space\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_A']==design[0]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_B']==design[1]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_C']==design[2]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_D']==design[3]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_E']==design[4]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_F']==design[5]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_G']==design[6]]\n",
    "        tset=pd.concat([tset,sub])\n",
    "    return tset,tcart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03b6018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "14.572017431259155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Load simulations\n",
    "comb_space=pd.read_csv(\"../data/combinatorial_space/combinatorial_space_pathway_A.csv\")\n",
    "\n",
    "#enzyme names and perturbation range\n",
    "enz_names=[\"vmax_forward_Enzyme_A\",\"vmax_forward_Enzyme_B\",\"vmax_forward_Enzyme_C\",\"vmax_forward_Enzyme_D\",\n",
    "           \"vmax_forward_Enzyme_E\",\"vmax_forward_Enzyme_F\",\"vmax_forward_Enzyme_G\"] #'vmax_forward_LDH_D',\n",
    "perturb_range=[0.25,0.5,1,1.5,2,4]\n",
    "designs,cart=sf.generate_perturbation_scheme(enz_names,perturb_range)\n",
    "\n",
    "\n",
    "\n",
    "N_runs_averaging=10\n",
    "topX=100\n",
    "a=time.time()\n",
    "\n",
    "\n",
    "results=run_intersection_benchmark(N_runs,N_runs_averaging,topX, N_designs)\n",
    "b=time.time()\n",
    "print(b-a)\n",
    "results=pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ad9cd",
   "metadata": {},
   "source": [
    "# Identify parameters for bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e9ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd\n",
    "regr_sgd = make_pipeline(StandardScaler(),SGDRegressor(loss=\"squared_error\",max_iter=1000, tol=1e-3))\n",
    "#regr_sgd.fit(train_x, train_y)\n",
    "#predict_sgd=regr_sgd.predict(test_X)\n",
    "\n",
    "#rf\n",
    "regr_rf = RandomForestRegressor()\n",
    "#regr_rf.fit(train_X, train_Y)\n",
    "#predict_rf=regr_rf.predict(test_X)\n",
    "\n",
    "#Elastic Net\n",
    "regr_elasticnet=ElasticNet()\n",
    "#regr_elasticnet.fit(train_X,train_Y)\n",
    "#predict_elasticnet=regr_elasticnet.predict(test_X)\n",
    "\n",
    "\n",
    "#Gradient boosting Regressor\n",
    "regr_GradBoostReg=GradientBoostingRegressor()\n",
    "#regr_GradBoostReg.fit(train_X,train_Y)\n",
    "#predict_GradBoostReg=regr_GradBoostReg.predict(test_X)\n",
    "\n",
    "# Neural Network\n",
    "regr_NN=MLPRegressor(max_iter=8000,activation=\"relu\", learning_rate=\"adaptive\")\n",
    "#regr_NN.fit(train_X,train_Y)\n",
    "#predict_NN=regr_NN.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e69b8801",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31414/4023978663.py\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     cv=5)\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mregr_GradBoostReg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val. score: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test score: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "regr_sgd.get_params()\n",
    "#no real parameters to set for this one, is linear\n",
    "\n",
    "regr_GradBoostReg.get_params()\n",
    "# min_samples_split (2), min_samples_leaf (1), max_depth (3),learning rate (0.001,0.01,0.1,0.2),n_estmators (100),\n",
    "\n",
    "regr_rf.get_params()\n",
    "# min_samples_split (2), min_samples_leaf (1), max_depth (3),n_estimators (100),\n",
    "\n",
    "regr_NN.get_params()\n",
    "#activation: 'relu',alpha ()\n",
    "#parameter_space = {\n",
    "#    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "#    'activation': ['tanh', 'relu'],\n",
    "#    'solver': ['sgd', 'adam'],\n",
    "#    'alpha': [0.0001, 0.05],\n",
    "#    'learning_rate': ['constant','adaptive'],\n",
    "#}\n",
    "\n",
    "\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "regr_GradBoostReg = BayesSearchCV(\n",
    "    MLPRegressor(),\n",
    "    {\n",
    "        \"min_samples_split\":(1,2,3,4,5,6),\n",
    "        \"min_samples_leaf\":(1,2,3,4,5,6),\n",
    "        \"hidden_layer_sizes\": (1,2,3,4,5),\n",
    "        \"learning_rate\":(0.001,0.01,0.1,0.2,0.3),\n",
    "        #'learning_rate': (0.01,0.2,0.4,0.6),\n",
    "        #'min_samples_split': (2,3,4)\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=5)\n",
    "\n",
    "regr_GradBoostReg.fit(train_x, train_y)\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24660c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
