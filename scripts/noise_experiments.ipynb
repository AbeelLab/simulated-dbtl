{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f553293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytfa.io.json import load_json_model\n",
    "from skimpy.io.yaml import  load_yaml_model\n",
    "from skimpy.analysis.oracle.load_pytfa_solution import load_concentrations, load_fluxes\n",
    "from skimpy.core.parameters import ParameterValues\n",
    "from skimpy.utils.namespace import *\n",
    "from skimpy.core.modifiers import *\n",
    "from skimpy.io.yaml import load_yaml_model\n",
    "from skimpy.core.reactor import Reactor\n",
    "from skimpy.analysis.oracle.load_pytfa_solution import load_concentrations, load_fluxes\n",
    "from skimpy.viz.plotting import timetrace_plot\n",
    "from pytfa.io.json import load_json_model\n",
    "from skimpy.io.yaml import load_yaml_model\n",
    "from skimpy.analysis.oracle.load_pytfa_solution import load_concentrations\n",
    "from skimpy.core.parameters import load_parameter_population\n",
    "from skimpy.simulations.reactor import make_batch_reactor\n",
    "from skimpy.core.solution import ODESolutionPopulation\n",
    "from skimpy.utils.namespace import *\n",
    "from skimpy.viz.escher import animate_fluxes, plot_fluxes\n",
    "import copy\n",
    "from skimpy.io.yaml import export_to_yaml\n",
    "from skimpy.analysis.ode.utils import make_flux_fun\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import seaborn as sns\n",
    "import skimpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import matplotlib\n",
    "import sys\n",
    "sys.path.insert(1, '../functions/')\n",
    "\n",
    "# benchmark functions\n",
    "import simulation_functions as sf\n",
    "import scenarios as sc\n",
    "import visualizations as vis\n",
    "import noise as noise\n",
    "\n",
    "\n",
    "#ML methods\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import  AdaBoostRegressor\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39e9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_methods(train_x,train_y,test_x,test_y):\n",
    "    \"wrapper s.t. we can call it iteratively for each scenario\"\n",
    "    #train test split\n",
    "    #X=['vmax_forward_Enzyme_A','vmax_forward_Enzyme_B',\"vmax_forward_Enzyme_C\",\n",
    "     #  \"vmax_forward_Enzyme_D\",\"vmax_forward_Enzyme_E\",\"vmax_forward_Enzyme_F\",\"vmax_forward_Enzyme_G\"]\n",
    "    #train_x=training_set_sc1[X]\n",
    "    #train_y=training_set_sc1['Enzyme_G']\n",
    "    #test_x=test_set_simulation[X]\n",
    "    #test_y=test_set_simulation['Enzyme_G']\n",
    "    \n",
    "    \n",
    "    #score_svr=regr_svr.score(test_x,test_y)\n",
    "    #print(\"Support Vector Regressor: \"+str(regr_svr.score(test_x,test_y)))\n",
    "    #sgd\n",
    "    regr_sgd = make_pipeline(StandardScaler(),SGDRegressor(loss=\"squared_error\",max_iter=1000, tol=1e-3))\n",
    "    regr_sgd.fit(train_x, train_y)\n",
    "    predict_sgd=regr_sgd.predict(test_x)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(test_y,predict_sgd)\n",
    "    score_sgd=r_value**2\n",
    "    \n",
    "\n",
    "    #print(\"Stochastic Gradient Descent Regressor: \"+str(regr_sgd.score(test_x,test_y)))\n",
    "    #rf\n",
    "    regr_rf = BayesSearchCV(\n",
    "    RandomForestRegressor(),\n",
    "    {\n",
    "        \"min_samples_split\":(2,3,4,5,6,7),\n",
    "        \"min_samples_leaf\":(2,3,4,5,6,7),\n",
    "        \"max_depth\": (1,2,3,4,5,7),\n",
    "    },\n",
    "    n_iter=15,\n",
    "    cv=5)\n",
    "    regr_rf.fit(train_x,train_y)\n",
    "    print(regr_rf.best_estimator_)\n",
    "    predict_rf=regr_rf.predict(test_x)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(test_y,predict_rf)\n",
    "    score_rf=r_value**2\n",
    "\n",
    "    \n",
    "    #Gradient boosting Regressor\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    regr_GradBoostReg = BayesSearchCV(\n",
    "    GradientBoostingRegressor(),\n",
    "    {\n",
    "        \"min_samples_split\":(2,3,4,5,6,7),\n",
    "        \"min_samples_leaf\":(2,3,4,5,6,7),\n",
    "        \"max_depth\": (1,2,3,4,5,7),\n",
    "        \"learning_rate\":(0.0001,0.001,0.01,0.1,0.2,0.3),\n",
    "        #'learning_rate': (0.01,0.2,0.4,0.6),\n",
    "        #'min_samples_split': (2,3,4)\n",
    "    },\n",
    "    n_iter=15,\n",
    "    cv=5)\n",
    "    \n",
    "    regr_GradBoostReg.fit(train_x, train_y)\n",
    "    print(regr_GradBoostReg.best_estimator_)\n",
    "    predict_GradBoostReg=regr_GradBoostReg.predict(test_x)\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = linregress(test_y,predict_GradBoostReg)\n",
    "    score_GradBoost=r_value**2\n",
    "    \n",
    "    #print(\"Gradient Boosting Regressor: \"+str(regr_GradBoostReg.score(test_x,test_y))) \n",
    "    # Neural Network\n",
    "    regr_NN = BayesSearchCV(\n",
    "    MLPRegressor(),\n",
    "    {\n",
    "        \"activation\":(\"logistic\",\"tanh\",\"relu\"),\n",
    "        \"alpha\":(0.0001,0.01),\n",
    "        \"max_iter\":(5000,8000),\n",
    "        \"hidden_layer_sizes\": (1,20),\n",
    "        \"learning_rate\":(\"invscaling\",\"constant\",\"adaptive\"),\n",
    "    },\n",
    "    n_iter=15,\n",
    "    cv=5)\n",
    "    regr_NN.fit(train_x,train_y)\n",
    "    print(regr_NN.best_estimator_)\n",
    "    predict_NN=regr_NN.predict(test_x)\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = linregress(test_y,predict_NN)\n",
    "    score_NN=r_value**2\n",
    "    \n",
    "    #print(\"Neural Network: \"+str(regr_NN.score(test_x,test_y))) \n",
    "    algorithms=[\"SGD\",\"RF\",\"GBR\",\"NN\"]\n",
    "    scores=[score_sgd,score_rf,score_GradBoost,score_NN]\n",
    "    scores=dict(zip(algorithms,scores))\n",
    "    return scores \n",
    "\n",
    "\n",
    "\n",
    "def find_set_designs(comb_space,tcart,enz_names):\n",
    "    \"\"\"finds the training or test set designs in the combinatorial space\n",
    "    Number of features has to be given\n",
    "    - combinatorial space\n",
    "    - cart of either the training scenario or the test set\n",
    "    - enzyme names\"\"\"\n",
    "    temp=0\n",
    "    tset = pd.DataFrame()\n",
    "    for design in tcart:\n",
    "        sub=comb_space\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_A']==design[0]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_B']==design[1]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_C']==design[2]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_D']==design[3]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_E']==design[4]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_F']==design[5]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_G']==design[6]]\n",
    "        tset=pd.concat([tset,sub])\n",
    "    return tset,tcart\n",
    "\n",
    "\n",
    "\n",
    "def find_set_designs(comb_space,tcart,enz_names):\n",
    "    \"\"\"finds the training or test set designs in the combinatorial space\n",
    "    Number of features has to be given\n",
    "    - combinatorial space\n",
    "    - cart of either the training scenario or the test set\n",
    "    - enzyme names\"\"\"\n",
    "    temp=0\n",
    "    tset = pd.DataFrame()\n",
    "    for design in tcart:\n",
    "        sub=comb_space\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_A']==design[0]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_B']==design[1]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_C']==design[2]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_D']==design[3]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_E']==design[4]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_F']==design[5]]\n",
    "        sub=sub.loc[sub['vmax_forward_Enzyme_G']==design[6]]\n",
    "        tset=pd.concat([tset,sub])\n",
    "    return tset,tcart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22189c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load simulations\n",
    "comb_space=pd.read_csv(\"../data/combinatorial_space/combinatorial_space_pathway_A.csv\")\n",
    "\n",
    "#enzyme names and perturbation range\n",
    "enz_names=[\"vmax_forward_Enzyme_A\",\"vmax_forward_Enzyme_B\",\"vmax_forward_Enzyme_C\",\"vmax_forward_Enzyme_D\",\n",
    "           \"vmax_forward_Enzyme_E\",\"vmax_forward_Enzyme_F\",\"vmax_forward_Enzyme_G\"] #'vmax_forward_LDH_D',\n",
    "perturb_range=[0.25,0.5,1,1.5,2,4]\n",
    "designs,cart=sf.generate_perturbation_scheme(enz_names,perturb_range)\n",
    "\n",
    "\n",
    "#Define the scenario \n",
    "scenario=\"equal\"\n",
    "N_designs=100\n",
    "if scenario==\"equal\":\n",
    "    sc1_designs,sc1_cart=sc.scenario1(perturb_range,N_designs,enz_names)\n",
    "elif scenario==\"radical\":\n",
    "    sc1_designs,sc1_cart=sc.scenario2(perturb_range,N_designs,enz_names)\n",
    "elif scenario==\"non-radical\":\n",
    "    sc1_designs,sc1_cart=sc.scenario3(perturb_range,N_designs,enz_names)\n",
    "\n",
    "\n",
    "training_scenario1,training_cart=find_set_designs(comb_space,sc1_cart,enz_names)  \n",
    "\n",
    "#add noise\n",
    "noise_G=noise.add_homoschedastic_noise(training_scenario1['Enzyme_G'],0.15)\n",
    "training_scenario1['Enzyme_G']=noise_G\n",
    "\n",
    "\n",
    "train_x=training_scenario1[enz_names]\n",
    "train_y=training_scenario1['Enzyme_G']\n",
    "test_x=comb_space[enz_names]\n",
    "test_y=comb_space['Enzyme_G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b8df60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_depth=3, min_samples_leaf=2, min_samples_split=5)\n",
      "GradientBoostingRegressor(max_depth=4, min_samples_leaf=4, min_samples_split=4)\n",
      "MLPRegressor(activation='logistic', alpha=0.008798313291568053,\n",
      "             hidden_layer_sizes=10, learning_rate='invscaling', max_iter=5046)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9697/652973896.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mML_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "scores=ML_methods(train_x,train_y,test_x,test_y)\n",
    "results.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac55bb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SGD': 0.36712088198513576,\n",
       " 'RF': 0.7097368585225489,\n",
       " 'GBR': 0.7958565497077473,\n",
       " 'NN': 0.6625886249440753}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9838df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
